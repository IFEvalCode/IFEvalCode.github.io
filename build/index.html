<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="description" content="IFEvalCode: Controlled Code Generation" />
  <meta name="keywords" content="Controlled Code Generation, Code LLM, Evaluation, Benchmark" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    IFEvalCode: Controlled Code Generation
  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || []

      function gtag() {
        dataLayer.push(arguments)
      }

      gtag("js", new Date())

      gtag("config", "G-PYVRSFMDRL")
    </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./css/bulma.min.css" />
  <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./css/bulma-slider.min.css" />
  <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./css/index.css" />
  <link rel="icon" href="./images/favicon.svg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              IFEvalCode: Controlled Code Generation
            </h1>
            
       <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Jian Yang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Wei Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Shukai Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Linzheng Chai</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Yingshui Tan</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="">Jiaheng Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Ge Zhang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="">Wangchunshu Zhou</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="">Guanglin Niu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Zhoujun Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">BinYuan Hui</a><sup></sup>,</span>
              <span class="author-block">
                <a href="">Junyang Lin</a><sup></sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CCSE, Beihang University</span>
              <span class="author-block"><sup>2</sup>MAP</span>
              <span class="author-block"><sup>3</sup>OPPO</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (Coming Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/CSJianYang/IFEvalCode"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="."
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Evaluation Data (Coming Soon)</span>
                  </a>
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="."
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>IFEvalCode-Instruct (Coming Soon)</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="leaderboard.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <!-- center the image -->
          <img src="./ifcode_images/intro.png" alt="Teaser" class="teaser-image center" width="100%" />
        </div>
        <!--
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">IFEvalCode</span> collects problems from periodic contests on <span
            class="dnerf">LeetCode</span>, <span class="dnerf">AtCoder</span>, and <span class="dnerf">Codeforces</span>
          platforms and uses them for constructing a holistic benchmark for evaluating Code LLMs across variety of
          code-related scenarios continuously over time.
        </h2>
        -->
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Code large language models (Code LLMs) have achieved significant advancements in various code-related tasks, particularly in code generation, where the code LLMs produce the target code from natural language descriptions. However, in realistic scenarios, users often expect the return code to strictly follow the given detailed requirements in many aspects (e.g. the style of code, the number of code lines, or the number of lines), instead of only requiring the correctness of the generated code. Controlled code generation means that the generated response from code LLMs should adhere to specific human guidelines or standards, whereas the LLM should have a strong instruction-following capability in the field of the code. In this paper, we propose forward constraints generation and backward constraints generation for controlled code generation to enhance the capability of LLM in following human instructions. Then, we build a multilingual benchmark IFEvalCode to evaluate the code instruction-following capability of the LLMs. IFEvalCode consists of 1.6K samples (Python, Java, Javascript, Typescript, Shell, Cpp, and C-sharp) and each test sample contains the Chinese and English query. Different from the existing code benchmarks, we separately design the test function to verify the correctness of the code (Corr.) and whether the generated code follows the human instruction (Instr.). Extensive experimental results on IFEvalCode of 40+ LLMs emphasize that closed-source LLMs still dominate in controllable code generation compared to open-source LLMs and the ability of LLMs to generate controllable code is far behind its ability to generate correct code
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>
  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Contamination</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">IFEvalCode</span> annotates problems with release dates, and thus allows evaluating
              models on problems released during a specific time period. Thus, for a newer model
              with a training-cutoff date <span class="dnerf">D</span>, we can evaluate it on problems released after
              <span class="dnerf">D</span> to measure its generalization on <i>unseen</i> problems.
            </p>
            <!-- side by side images->
            <div class="columns is-centered">
              <img src="./images/contamination1.png" alt="Code Generation Live Evaluation" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/contamination2.png" alt="Test Output Prediction Live Evaluation" class="teaser-image"
                width="48%" class="center" />
            </div>

            <p>
              The above plots depict the performance of models on code generation and test output prediction scenarios
              on problems released over different months. We find that <span class="dnerf">DeepSeek</span> models
              exhibit a stark drop in performance on LeetCode problems released since September 2023, its release date,
              indicating that the earlier problems might be contaminated. In contrast, for
              <span class="dnerf">GPT</span>
              models, the performance is relatively stable across different months.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Holistic Evaluation and Open vs Closed Models</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf">IFEvalCode</span> evaluates models on a variety of code-related scenarios, such as
              code generation, self-repair, test output prediction, and code execution. We find that while model
              performances
              are correlated across different scenarios, there relative performances and ordering can vary (left
              figure).
              For instance, <span class="dnerf">Claude-3-Opus</span> overtakes <span class="dnerf">GPT-4-turbo</span> in
              the
              test output prediction scenario, but not in the code generation scenario. Similarly,
              <span class="dnerf">Mistral-Large</span>
              performs considerably better on natural language reasoning tasks like test output prediction and code
              execution.
            </p>
            <br />
            <div class="columns is-centered">
              <img src="./images/tasks_radar.png" alt="Holistic Evaluation" class="teaser-image" width="44%"
                height="44%" class="center" />
              <img src="./images/lc_barchart.png" alt="Open vesus Closed Models" class="teaser-image" width="48%"
                class="center" />

              <!-- <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image" width="55%"
                height="55%" class="center" /> ->
            </div>
            <p>
              We compare the performance of open access models with closed api-access models on <span
                class="dnerf">IFEvalCode</span> and find that generally the closed api-access models outperform the
              open models. Particularly, the only open models that surpass the barrier are fine-tuned variants of large
              (30+B parameter) models.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Potential Overfitting in HumanEval</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We also find that models that perform well on <span class="dnerf">HumanEval</span> might be overfitting on
              the benchmark. Particularly, the models are separated into two clusters depicted
              by the green and red shaded region in the right scatterplot. The models in the green region perform
              similarly on
              <span class="dnerf">HumanEval</span> and <span class="dnerf">LCB-Easy</span>, while the models in the red
              region
              perform well on <span class="dnerf">HumanEval</span> but lag behind on <span
                class="dnerf">LCB-Easy</span>.
              For instance, <span class="dnerf">DS-Ins-1.3B</span> model outperforms <span
                class="dnerf">Gemini-Pro</span> and <span class="dnerf">Claude-Ins-1</span> but performs considerably
              worse on <span class="dnerf">LCB-Easy</span>.
              Interestingly, the models in the red region are mostly fine-tuned variants of open access models. On the
              other hand, base models and most of the closed api-access models lie in the green region. This highlights
              a potential lack of diverse fine-tuning data being employed by the open source community and the need for
              optimizing models for a broader set of code-related tasks.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Comparisions</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/codegen_performance.png" alt="Code Generation Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/repair_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div>
            <div class="columns is-centered">
              <img src="./images/testgen_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
              <img src="./images/execution_performance.png" alt="Code Execution Performance" class="teaser-image"
                width="48%" class="center" />
            </div>
            <p>
              The above plots depict the performance of models on different scenarios considered in <span
                class="dnerf">IFEvalCode</span>. We find that <span class="dnerf">GPT-4-turbo</span> and <span
                class="dnerf">Claude-3-Opus</span> models perform best across different scenarios. Among open source
              models, <span class="dnerf">DS-Ins-33B</span> and <span class="dnerf">Phind-34B</span> perform the best.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Submitting Custom Models</h2>
          <div class="content has-text-justified">
            <p>
              To submit models to the leaderboard, you can run the evaluation using the evaluation scripts in
              <a href="https://github.com/IFEvalCode/IFEvalCode">GitHub</a>. Once you have the results,
              you can fill out <a href="https://forms.gle/h2abvAHh6UnhWzzd9">this form</a>. You will need to fill out
              model details and provide the generated evaluation file with model generations and pass@1 scores. We will
              review the submission and add the model to the leaderboard accordingly.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Potential Overfitting in HumanEval</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/lcb_vs_he.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We also find that models that perform well on <span class="dnerf">HumanEval</span> might be overfitting on
              the benchmark. Particularly, the models are separated into two clusters depicted
              by the green and red shaded region in the right scatterplot. The models in the green region perform
              similarly on
              <span class="dnerf">HumanEval</span> and <span class="dnerf">LCB-Easy</span>, while the models in the red
              region
              perform well on <span class="dnerf">HumanEval</span> but lag behind on <span
                class="dnerf">LCB-Easy</span>.
              For instance, <span class="dnerf">DS-Ins-1.3B</span> model outperforms <span
                class="dnerf">Gemini-Pro</span> and <span class="dnerf">Claude-Ins-1</span> but performs considerably
              worse on <span class="dnerf">LCB-Easy</span>.
              Interestingly, the models in the red region are mostly fine-tuned variants of open access models. On the
              other hand, base models and most of the closed api-access models lie in the green region. This highlights
              a potential lack of diverse fine-tuning data being employed by the open source community and the need for
              optimizing models for a broader set of code-related tasks.
            </p>
          </div>
        </div>
      </div>
  </section>
  -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Statistics</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./ifcode_images/statistics.png" alt="statistics" class="teaser-image center" width="50%" height="50%" />
            </div>
            <p>
              IFEvalCode consists of 1.6K problems. Each task contains about 1.3K questions, with more than 60 problems in each language. Each sample in IFEvalCode includes (en_question, zh_question, check_instruction, check_correctness), where `check_instruction` is used to check the code correctness and `check_correctness` judges whether the generated code follows the human instruction. We calculate the length of the question using the Qwen2.5-Coder tokenizer and count the number of instruction constraints for each sample. Each question contains 3 constraints and 100 tokens on average, where each prompt contains a problem description and the corresponding function or class signature.
            </p>
            <div class="columns is-centered">
              <img src="./ifcode_images/domain_and_instruction.png" alt="IFEvalCode Domain and Instruction" class="teaser-image center" width="140%"
                height="140%" />
            </div>
            <p>
              We show domain types and instruction types in IFEvalCode.
            </p>
            <div class="columns is-centered">
              <img src="./ifcode_images/examples.png" alt="IFEvalCode Examples" class="teaser-image center" width="120%" height="80%" />
            </div>
            <p>
              We show the examples of verifiable instruction with `check_instruction` in IFEvalCode.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Comparison with Other Code Benchmarks</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./ifcode_images/comparison.png" alt="statistics" class="teaser-image center" width="100%"
                height="80%" />
            </div>
            <p>
              we compare IFEvalCode with other code evaluation benchmarks in 6 aspects. Considering the evaluation cost and effectiveness, IFEvalCode has 1620 samples for 8 programming languages and 2 human languages. Specially, we introduce the two part of unit tests to check the code correctness and whether the code follows the human instruction.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> IFEvalCode Construction & Quality Control</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./ifcode_images/pipeline.png" alt="IFEvalCode Construction & Quality Control" class="teaser-image center" width="100%" height="100%" />
            </div>
            <p>
              We begin by collecting code snippets and code-related documents from websites and recruit 6 computer science graduates as annotators. The annotators must follow the provided guidelines (1) ensure the diversity of the questions. (2) the question and instruction constraints should be challenging for existing LLMs. (3) translate the English question into the Chinese question. To increase the difficulty of IFEvalCode, the annotator filter out the questions, which half LLMs (GPT4o, DeepSeek-V3, Claude3.7, etc) can correctly answer.
            </p>
          </div>
        </div>
      </div>  
  </section>
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">mCoder</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/3_framework.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              
            </p>
          </div>
        </div>
      </div>
</section> -->
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Further Analysis</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./ifcode_images/distribution.png" alt="IFEvalCode Distribution" class="teaser-image center" width="100%" height="100%" />
            </div>
            <p>
              We plot the trends with the number of constraints increasing.
            </p>
          </div>


          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./ifcode_images/venn_diagram.png" alt="IFEvalCode Venn Diagram" class="teaser-image center" width="100%" height="100%" />
            </div>
            <p>
              We show a grid of four-set Venn diagrams—one subplot per programming language—where each diagram partitions the model’s outputs into the following four evaluation sets: (1) Chinese‐prompt correctness, (2) Chinese‐prompt instruction compliance, (3) English‐prompt correctness, and (4) English‐prompt instruction compliance. By inspecting the overlaps, we can immediately see several consistent patterns across all eight languages: Correctness dominates. Both the Chinese‐correctness and English‐correctness sets are substantially larger than their corresponding instruction‐compliance sets, indicating that LLMs more often produce functionally correct code than code that also respects the extra constraints. Cross-language consistency in correctness. The overlap between Chinese‐correctness and English‐correctness is very large in every language, showing that whether the prompt is given in Chinese or English has little effect on a model’s ability to generate a passing solution. Instruction compliance is more fragile. The Chinese‐instruction and English‐instruction sets are much smaller, and their overlap is modest. In other words, even if a model is correct in both languages, it frequently fails to satisfy the same set of style or structural constraints when the prompt language switches. The smallest region is the four-way intersection. Only a small fraction of examples are simultaneously correct and compliant in both Chinese and English, demonstrating that achieving full controlled generation across languages remains the hardest challenge.
            </p>
          </div>
        </div>
      </div>
</section>

<!-- 
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Performance in Code Completion Tasks</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/5_radar_class_result.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              It can be observed that code LLMs generally perform better in object-oriented and multi-paradigm
              programming languages (high-resource languages), while perform worse in functional and procedural programming languages (low-resource languages).
              In areas like web development and scientific computing, the gap between open-source and closed-source models is narrowing. However, for application scenarios, there is still a substantial gap
              between open-source models and the closed-source GPT-4 series in low-resource languages related to scripting, mobile development, and educational research. mCoder performs superior over multiple same-size models and even some larger open-source models.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Unbalance in Different Languages</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/6_cross_dataset.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We compare the results of several open-source models on the Multipl-E multilingual benchmark with corresponding languages on IFEvalCode.
              We obtained scores for 11 programming languages (including Python, Java, JavaScript, C++, PHP, Rust, Swift, R, Lua, Racket, and Julia) from the BigCode
              leaderboard. As shown in Figure (1), due to the simplicity of Python language tasks in this dataset, many models exhibit significant score discrepancies between the
              two benchmarks. By examining Figure (2) and (3), it becomes evident that all models demonstrate
              consistent multilingual capabilities between Multipl-E and IFEvalCode. However, Figure (2) highlights
              a majority of models within the blue circle, indicating that the current state-of-the-art performance
              of most models primarily lies in high-resource languages like Python, while their proficiency in
              low-resource languages awaits further exploration and enhancement.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Cross-lingual Transfer</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/7_cross-lingual-transfer.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              We fine-tune the CodeQwen-1.5 model using Python-only data in IFEvalCode-Instruct
              and compare it with mCoder. CodeQwen-1.5 performs well in most high-resource languages, but CodeQwen without alignment exhibits unsatisfactory results in
              some low-resource languages due to the inability to follow instructions. As such, with fine-tuning using only Python data,
              CodeQwen-1.5-Python improves significantly across most languages. It shows that the CodeQwen foundation model already possesses strong coding capabilities
              but lacks adequate instruction-following skills. Therefore, fine-tuning with Python-only data can still effectively transfer instruction-following abilities to other languages,
              resulting in superior multilingual performance.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">CodeQwen-1.5-Chat performance on IFEvalCode for problems of different difficulty levels.</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/8_level_statistics.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              Based on algorithmic complexity, we classify IFEvalCode into three levels (Easy/Medium/Hard).
In Figure 1, we conduct a statistical analysis of CodeQwen-1.5-Chat's performance on code generation tasks
across various languages. For most languages, the codeLLM can answer the majority of easy questions but struggles with medium and hard ones.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Generation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/9_bench_completion_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including function name, function description, and function call cases),a reference solution, and
              a test cases part. Left Figure: an example of the Lisp language. Middle Figure: a file processing
              programming task in AWK language. During the evaluation, the corresponding file processing result
              by the generated code will be compared with the reference answer. Right Figure: an example of the R language.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Explanation</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/10_bench_explain_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part (including a complete function),
              a reference Explanation. Left Figure: an example of the Kotlin language. Middle Figure: an example of
              the Lua language. Right Figure: an example of the HTML language.
            </p>
          </div>
        </div>
      </div>
</section>
<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples of Multilingual Completion</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/11_bench_fim_cases.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              The data mainly consists of an instruction part
              (including an incomplete function ), a reference complete code solution and test cases. Left Figure: an
              span completion example of the C++ language. Middle Figure: a single-line completion example of
              the Rust language. Right Figure: a multiple-line completion example of the Shell language.
            </p>
          </div>
        </div>
      </div>
</section> -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Optimization Details</h2>
        <div class="content has-text-justified">
          <div class="columns is-centered">
            <img src="./images/12_ts.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
              height="80%" />
          </div>
          <p>
            All mCoder models are fine-tuned using 8 NVIDIA A800-80GB GPUs. The models are trained for
            2 epochs with a cosine scheduler, starting at a learning rate of 2e-5 and incorporating a 3% warmup
            phase. Training a model takes about 5 hours. We used AdamW (Loshchilov and Hutter, 2017) as
            the optimizer and a batch size of 512 with a sequence truncation length of 4096. We use PyTorch’s
            Fully Sharded Data Parallel (FSDP) to perform distributed training of the model, and use gradient
            checkpointing technology and gradient accumulation to save memory and achieve training with a
            larger batch size.
          </p>
        </div>
      </div>
    </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Analysis of Language Representations</h2>
          <div class="content has-text-justified">
            <div class="columns is-centered">
              <img src="./images/12_cluster_merge.png" alt="HumanEval Overfitting" class="teaser-image center" width="80%"
                height="80%" />
            </div>
            <p>
              As shown in the Figure, we analyzed the programming languages in the IFEvalCode from their presentation perspective.
              We used CodeBERT to extract code representations from code snippets in IFEvalCode.
              The figure clearly shows that languages with similar syntax have closely related representations.
              For example, other functional programming languages similar to CommonLisp, as well as C, C++, Java, and scripting
              languages, exhibit high grammar similarity.
            </p>
          </div>
        </div>
      </div>
</section> -->

<!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>
-->
  <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Please reach out to <a href="jiaya@buaa.edu.cn">jiaya@buaa.edu.cn</a> for questions or
              feedback on IFEvalCode. We are also open to collaborations and suggestions for new scenarios to add to
              the benchmark. Finally, IFEvalCode provides one axis of LLM coding evaluations and we recommend the
              following leaderboards for measuring code LM ability on various coding tasks, such as
              <a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench Leaderboard</a>,
              <a href="https://evalplus.github.io/leaderboard.html">EvalPlus Leaderboard</a>,
              <a href="https://crux-eval.github.io/leaderboard.html">CruxEval Leaderboard</a>,
              <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena Leaderboard</a>,
              <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard</a>,
              <a href="https://infi-coder.github.io/inficoder-eval/">InfiCoder-Eval</a>, and
              <a href="https://leaderboard.tabbyml.com/">TabbyML Leaderboard</a>.
            </p>
            <p>
              The source code from this website is borrowed from <a
                href="https://github.com/LiveCodeBench/livecodebench.github.io">LiveCodeBench</a>!
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
